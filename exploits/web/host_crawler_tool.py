from datetime import datetime
from typing import Type, Any
from urllib.parse import urlparse, urlunparse

import validators
from crewai_tools import BaseTool
from pydantic.v1 import BaseModel, Field
from sqlalchemy import exc

from helpers import fingers
from helpers.crawler import crawl_host
from persistence.database import DB
from config import logger
from persistence.orm import WebInfo, Cdn


class HostCrawlerToolSchema(BaseModel):
    """HostCrawlerToolSchema 的查询参数"""
    host: str = Field(..., description="host地址, 一个host或者完整的url地址，例如：`example.com`或`http://example.com`")


class HostCrawlerTool(BaseTool):
    name: str = "HostCrawler"
    description: str = "根据host地址，获取网页的图标、标题、响应头、HTML正文等信息，同时根据指纹判断其应用信息。"
    args_schema: Type[BaseModel] = HostCrawlerToolSchema
    db: DB | None = None

    class Config:
        arbitrary_types_allowed = True

    def __init__(self, db: DB):
        super().__init__()
        self.db = db
        logger.info("初始化工具 HostCrawler")

    def _run(
            self,
            **kwargs: Any,
    ) -> Any:
        host = kwargs.pop('host', "")
        if host == "":
            return "host为空"

        if validators.domain(host) is True:
            url_parsed = urlparse("http://" + host)
            host = urlunparse((url_parsed.scheme, url_parsed.netloc, url_parsed.path, '', '', ''))

        if validators.url(host) is False:
            return "host地址不合法"
        try:
            logger.info("HostCrawler: {}", host)
            now = datetime.now()
            htmls = crawl_host(host)
            if len(htmls) == 0:
                return "获取失败"
            try:
                with self.db.DBSession() as session:
                    for html in htmls:
                        matched = fingers.Match(html.headers, html.body, [i.mmh3hash for i in html.favicons],
                                                [i.md5hash for i in html.favicons])
                        webinfodb = WebInfo()
                        webinfodb.host = html.host
                        webinfodb.schema = html.schema
                        webinfodb.url = html.url
                        webinfodb.current_redirects = html.current_redirects
                        webinfodb.redirect_to = html.redirect_to
                        webinfodb.ip = html.ip
                        ipcdn = session.query(Cdn).filter(Cdn.cidr.op('>>')(html.ip)).first()
                        if ipcdn is not None:
                            webinfodb.ip_cdn = ipcdn.organization
                        webinfodb.port = html.port
                        webinfodb.title = html.title
                        webinfodb.status = html.status
                        webinfodb.headers = html.headers
                        webinfodb.favicons = [favicon.to_dict() for favicon in html.favicons]
                        webinfodb.body = html.body
                        webinfodb.certs = html.certs
                        webinfodb.created = now
                        webinfodb.finger_prints = [match.to_dict() for match in matched]
                        webinfodb.source = self.name

                        session.add(webinfodb)
                    session.commit()
            except exc.SQLAlchemyError as e:
                logger.error("数据库错误: {}", e)
                return "数据库错误"
            except Exception as e:
                logger.error("其他错误: {}", e)
                return f"其他错误: {e}"
            return f"共发现{len(htmls)}个特征"
        except Exception as e:
            logger.error("获取失败: {}", e)
            return f"获取失败: {e}"
